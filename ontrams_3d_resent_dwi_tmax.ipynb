{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dependencies\n",
    "- Change oversample indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install tabulate\n",
    "#! pip install classification-models-3D\n",
    "#! pip install keras_applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# from PIL import Image\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# from scipy import ndimage\n",
    "from scipy import ndimage\n",
    "from scipy.special import expit, logit\n",
    "\n",
    "from skimage import exposure\n",
    "import sklearn.metrics as skm\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Tensorflow/Keras\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.utils import to_categorical\n",
    "# from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "#from classification_models_3D.tfkeras import Classifiers\n",
    "\n",
    "# Own functions\n",
    "from functions.plot_slices import plot_slices\n",
    "# ontram functions\n",
    "from k_ontram_functions.ontram import ontram\n",
    "from k_ontram_functions.ontram_loss import ontram_loss\n",
    "from k_ontram_functions.ontram_metrics import ontram_acc, ontram_auc\n",
    "from k_ontram_functions.ontram_predict import predict_ontram, get_parameters\n",
    "\n",
    "from functions.augmentation3d import zoom, rotate, flip, shift\n",
    "from functions.augmentation3d_2modi import zoom2, rotate2, flip2, shift2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the path + output path:\n",
    "# os.getcwd()\n",
    "DIR = \"/tf/notebooks/\"\n",
    "DATA_DIR = \"hezo/stroke_bern/data/\"\n",
    "OUTPUT_DIR = \"/tf/notebooks/hezo/stroke_bern/callbacks/ontrams_3d_resent_dwi_tmax/\"\n",
    "OUTPUT_DIR_DWI = \"/tf/notebooks/hezo/stroke_bern/callbacks/ontrams_3d_resent_clinical_dwi/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(DIR + DATA_DIR + \"data_bern_25_11_2020_preprocessed.h5\", \"r\") as h5:\n",
    "    X_dwi0 = h5[\"X_dwi\"][:]\n",
    "    X_tmax0 = h5[\"X_tmax\"][:]\n",
    "print(X_dwi0.shape, X_dwi0.min(), X_dwi0.max(), X_dwi0.mean(), X_dwi0.std())\n",
    "print(X_tmax0.shape, X_tmax0.min(), X_tmax0.max(), X_tmax0.mean(), X_tmax0.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_slices(X_dwi0[0], 0, \"axial\", modality = \"DWI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_slices(X_tmax0[0], 0, \"axial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapt image size for ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reshape the data to the correct dimension\n",
    "dim = (128, 128, 20, 1)\n",
    "\n",
    "X_dwi = np.empty((len(X_dwi0), 128, 128, 20, 3))\n",
    "for i in range(len(X_dwi0)):\n",
    "    scaling_factor = [dim[0]/X_dwi0[i].shape[0], dim[1]/X_dwi0[i].shape[1], dim[2]/X_dwi0[i].shape[2], dim[3]/X_dwi0[i].shape[3]]\n",
    "    X_dwi[i,:,:,:,:] = ndimage.zoom(X_dwi0[i], scaling_factor, order = 1)\n",
    "X_dwi[:,:,:,:,1] = X_dwi[:,:,:,:,0]\n",
    "X_dwi[:,:,:,:,2] = X_dwi[:,:,:,:,0]\n",
    "\n",
    "plot_slices(X_dwi[0], 0, \"axial\", \"DWI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reshape the data to the correct dimension\n",
    "dim = (128, 128, 20, 3)\n",
    "\n",
    "X_tmax = np.empty((len(X_tmax0), 128, 128, 20, 3))\n",
    "for i in range(len(X_tmax0)):\n",
    "    scaling_factor = [dim[0]/X_tmax0[i].shape[0], dim[1]/X_tmax0[i].shape[1], dim[2]/X_tmax0[i].shape[2], dim[3]/X_tmax0[i].shape[3]]\n",
    "    X_tmax[i,:,:,:,:] = ndimage.zoom(X_tmax0[i], scaling_factor, order = 1)\n",
    "\n",
    "plot_slices(X_tmax[0], 0, \"axial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two modalities\n",
    "X = np.concatenate((X_dwi.reshape((222,128,128,20,3,1)),\n",
    "                    X_tmax.reshape((222,128,128,20,3,1))), axis = 5)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_dwi0\n",
    "del X_tmax0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import clinical and patient data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv(DIR + DATA_DIR + \"data_bern_25_11_2020_dwi.csv\")\n",
    "# dat.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add image information to define the expert model as in the paper with Janne\n",
    "dat1 = pd.read_csv(DIR + DATA_DIR + \"data_wide_all_nihss.csv\", sep = \",\")\n",
    "# dat1.head(3)\n",
    "\n",
    "# check if the patient IDs between datasets match\n",
    "print(all(dat.p_id.values == dat1.p_id.values))\n",
    "\n",
    "# attach values \n",
    "dat[\"S_Medm_rbf\"] = dat1.S_Medm_rbf.values\n",
    "dat[\"volume_adc\"] = dat1.volume_adc.values\n",
    "dat[\"volume_tar\"] = dat1.volume_tar.values\n",
    "dat[\"infarct_side\"] = dat1.infarct_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Size of the dataframe: same as the images\n",
    "dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define binary mRS\n",
    "dat[\"mrs_3months_binary\"] = 0\n",
    "dat.loc[dat.mrs_3months <= 2, \"mrs_3months_binary\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dat.mrs_3months, bins = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.hist(dat.mrs_3months_binary, bins = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAs?\n",
    "[dat.age.isnull().sum(), \n",
    " dat.nihss_bl.isnull().sum(),  \n",
    " dat.sys_bloodpressure_bl.isnull().sum(),\n",
    " dat.rf_diabetes.isnull().sum(), \n",
    " dat.rf_hypertonia.isnull().sum(), \n",
    " dat.rf_smoker.isnull().sum(),  \n",
    " dat.rf_tia_stroke.isnull().sum(), \n",
    " dat.lyse.isnull().sum(),\n",
    " dat.time_to_groin_puncture.isnull().sum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # simple imputation: replace all missing values with the mode of the column\n",
    "# for column in dat.columns:\n",
    "#     dat[column].fillna(dat[column].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the imputed data from the last project\n",
    "train0 = pd.read_csv(DIR + DATA_DIR + \"train_imputed_all1.csv\", sep = \",\")\n",
    "train1 = pd.read_csv(DIR + DATA_DIR + \"train_imputed_all2.csv\", sep = \",\")\n",
    "train2 = pd.read_csv(DIR + DATA_DIR + \"train_imputed_all3.csv\", sep = \",\")\n",
    "train3 = pd.read_csv(DIR + DATA_DIR + \"train_imputed_all4.csv\", sep = \",\")\n",
    "train4 = pd.read_csv(DIR + DATA_DIR + \"train_imputed_all5.csv\", sep = \",\")\n",
    "\n",
    "test0 = pd.read_csv(DIR + DATA_DIR + \"test_imputed_all1.csv\", sep = \",\")\n",
    "test1 = pd.read_csv(DIR + DATA_DIR + \"test_imputed_all2.csv\", sep = \",\")\n",
    "test2 = pd.read_csv(DIR + DATA_DIR + \"test_imputed_all3.csv\", sep = \",\")\n",
    "test3 = pd.read_csv(DIR + DATA_DIR + \"test_imputed_all4.csv\", sep = \",\")\n",
    "test4 = pd.read_csv(DIR + DATA_DIR + \"test_imputed_all5.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if I can compare the datasets: works\n",
    "# print(all(train0.age.values.round(2) == dat.age.values[train0.index-1].round(2)),\n",
    "# all(train1.age.values.round(2) == dat.age.values[train1.index-1].round(2)),\n",
    "# all(train2.age.values.round(2) == dat.age.values[train2.index-1].round(2)),\n",
    "# all(train3.age.values.round(2) == dat.age.values[train3.index-1].round(2)),\n",
    "# all(train4.age.values.round(2) == dat.age.values[train4.index-1].round(2)))\n",
    "# \n",
    "# print(all(test0.age.values.round(2) == dat.age.values[test0.index-1].round(2)),\n",
    "# all(test1.age.values.round(2) == dat.age.values[test1.index-1].round(2)),\n",
    "# all(test2.age.values.round(2) == dat.age.values[test2.index-1].round(2)),\n",
    "# all(test3.age.values.round(2) == dat.age.values[test3.index-1].round(2)),\n",
    "# all(test4.age.values.round(2) == dat.age.values[test4.index-1].round(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [train0, train1, train2, train3, train4]\n",
    "test = [test0, test1, test2, test3, test4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training and test indices for CV later to impute during CV\n",
    "train_idxes0 = [train0.index.values, train1.index.values, train2.index.values, train3.index.values, train4.index.values]\n",
    "test_idxes = [test0.index.values, test1.index.values, test2.index.values, test3.index.values, test4.index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_seed = 3004\n",
    "\n",
    "# define validation data\n",
    "train_idxes = []\n",
    "valid_idxes = []\n",
    "for i in range(5):\n",
    "    np.random.seed(my_seed)\n",
    "    train_idx, valid_idx = train_test_split(train_idxes0[i], test_size = int(len(train_idxes0[i])*0.15))\n",
    "    valid_idxes.append(np.sort(valid_idx))\n",
    "    train_idxes.append(np.sort(train_idx))\n",
    "    my_seed += 1\n",
    "    \n",
    "# define datasets: train and validation\n",
    "train_old = train\n",
    "train = []\n",
    "valid = []\n",
    "for f in range(5):\n",
    "    train.append(train_old[f].loc[train_old[f].index.isin(train_idxes[f]),:])\n",
    "    valid.append(train_old[f].loc[train_old[f].index.isin(valid_idxes[f]),:])\n",
    "\n",
    "# get patient IDs\n",
    "for i in range(5):\n",
    "    train[i] = train[i].assign(p_id = dat.p_id[train_idxes[i]-1].values)\n",
    "    valid[i] = valid[i].assign(p_id = dat.p_id[valid_idxes[i]-1].values)\n",
    "    test[i] = test[i].assign(p_id = dat.p_id[test_idxes[i]-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if indices overlap\n",
    "# np.sort(train[0].index.values)\n",
    "# np.sort(valid[0].index.values)\n",
    "# np.sort(test[0].index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  check if image data still corresponds to tables --> works\n",
    "# for i in range(5):\n",
    "#     print(all(train[i].age.values.round(2) == dat.age.values[train_idxes[i]-1].round(2)))\n",
    "#     print(all(valid[i].age.values.round(2) == dat.age.values[valid_idxes[i]-1].round(2)))\n",
    "#     print(all(test[i].age.values.round(2) == dat.age.values[test_idxes[i]-1].round(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y = np.array(dat.mrs_3months)\n",
    "print(Y.shape)\n",
    "Y = to_categorical(Y)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for the intercept function: C = number of classes\n",
    "def mod_baseline(C):\n",
    "    mod = keras.Sequential(name = \"mod_baseline\")\n",
    "    mod.add(keras.Input(shape = (1, )))\n",
    "    mod.add(keras.layers.Dense(C - 1, activation = \"linear\", use_bias = False))\n",
    "    return mod\n",
    "\n",
    "# Model for linear shift terms\n",
    "def mod_linear_shift(x):\n",
    "    mod = keras.Sequential(name = \"mod_linear_shift\")\n",
    "    mod.add(keras.Input(shape = (x, )))\n",
    "    mod.add(keras.layers.Dense(1, activation = \"linear\", use_bias = False))\n",
    "    return mod\n",
    "\n",
    "# Model for complex shift terms\n",
    "def mod_complex_shift(x):\n",
    "    mod = keras.Sequential(name = \"mod_complex_shift\")\n",
    "    mod.add(keras.Input(shape = (x, )))\n",
    "    mod.add(keras.layers.Dense(8, activation = \"relu\"))\n",
    "    mod.add(keras.layers.Dense(8, activation = \"relu\"))\n",
    "    mod.add(keras.layers.Dense(1, activation = \"linear\", use_bias = False))\n",
    "    return mod  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_models_3D_master.classification_models_3D_master.classification_models_3D.tfkeras import Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet with pretrained weights\n",
    "def img_model(nout, last_layer_activation = \"linear\"):\n",
    "    ResNet50, preprocess_input = Classifiers.get('resnet50')\n",
    "    base_model = ResNet50(\n",
    "        input_shape=(128, 128, 20, 3), \n",
    "        weights='imagenet', \n",
    "        include_top = False)\n",
    "\n",
    "    # add a global average pooling layer and the dense part and define model\n",
    "    x = base_model.output\n",
    "    x = keras.layers.GlobalAveragePooling3D()(x)\n",
    "    x = keras.layers.Dense(128, name=\"fc1\")(x)\n",
    "    x = keras.layers.Dense(128, name = \"fc2\")(x)\n",
    "    x = keras.layers.Dense(nout, name=\"output\")(x)\n",
    "    predictions = keras.layers.Activation(last_layer_activation, name='output_activation')(x)\n",
    "    \n",
    "    return keras.Model(inputs=base_model.input, outputs=predictions)\n",
    "mod = img_model(1, \"linear\")\n",
    "mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_models_3D.tfkeras import Classifiers\n",
    "\n",
    "def img_model(nout, last_layer_activation = \"linear\"):\n",
    "    ResNet50, preprocess_input = Classifiers.get('resnet50')\n",
    "    base_model = ResNet50(\n",
    "        input_shape=(128, 128, 20, 3), \n",
    "        weights='imagenet', \n",
    "        include_top = False)\n",
    "\n",
    "    # add a global average pooling layer and the dense part and define model\n",
    "    x = base_model.output\n",
    "    x = keras.layers.GlobalAveragePooling3D()(x)\n",
    "    x = keras.layers.Dense(128, name=\"fc1\")(x)\n",
    "    x = keras.layers.Dense(128, name = \"fc2\")(x)\n",
    "    x = keras.layers.Dense(nout, name=\"output\")(x)\n",
    "    predictions = keras.layers.Activation(last_layer_activation, name='output_activation')(x)\n",
    "    \n",
    "    return keras.Model(inputs=base_model.input, outputs=predictions)\n",
    "mod = img_model(1, \"linear\")\n",
    "mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras import backend as K\n",
    "# \n",
    "# def conv_part(in_):\n",
    "#     x = layers.Convolution3D(32, kernel_size=(3, 3, 3), padding = 'same', activation = 'relu', \n",
    "#                              kernel_initializer = 'he_normal')(in_)\n",
    "#     x = layers.BatchNormalization(center=True, scale=True)(x)\n",
    "#     x = layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "#     x = layers.Convolution3D(32, kernel_size=(3, 3, 3), padding = 'same', activation = 'relu', \n",
    "#                              kernel_initializer = 'he_normal')(x)\n",
    "#     x = layers.BatchNormalization(center=True, scale=True)(x)\n",
    "#     x = layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "#     x = layers.Convolution3D(64, kernel_size=(3, 3, 3), padding = 'same', activation = 'relu', \n",
    "#                              kernel_initializer = 'he_normal')(x)\n",
    "#     x = layers.BatchNormalization(center=True, scale=True)(x)\n",
    "#     x = layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "#     x = layers.Convolution3D(64, kernel_size=(3, 3, 3), padding = 'same', activation = 'relu', \n",
    "#                              kernel_initializer = 'he_normal')(x)\n",
    "#     x = layers.BatchNormalization(center=True, scale=True)(x)\n",
    "#     x = layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "#     x = layers.Convolution3D(64, kernel_size=(3, 3, 3), padding = 'same', activation = 'relu', \n",
    "#                              kernel_initializer = 'he_normal')(x)\n",
    "#     x = layers.BatchNormalization(center=True, scale=True)(x)\n",
    "#     x = layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "#     x = layers.Flatten()(x)\n",
    "#     return x\n",
    "# \n",
    "# # input is supposed to be 6D: None, 2, 128, 128, 64, 3 [batch_size, DWI/tmax, pixelx, pixely, pixelz, color]\n",
    "# def par_img_model(input_shape, output_shape, input_name, activation = \"linear\"):\n",
    "#     in_ = keras.Input(shape = input_shape, name = input_name)\n",
    "#     # split the input:\n",
    "#     dwi_in_ = layers.Lambda(lambda x: x[:,:,:,:,:,0])(in_)\n",
    "#     tmax_in_ = layers.Lambda(lambda x: x[:,:,:,:,:,1])(in_)\n",
    "#     x_dwi = conv_part(dwi_in_)\n",
    "#     x_tmax = conv_part(tmax_in_)\n",
    "#     x = layers.concatenate((x_dwi, x_tmax))\n",
    "#     x = layers.Dense(128, activation = 'relu', kernel_initializer = 'he_normal')(x)\n",
    "#     x = layers.BatchNormalization(center=True, scale=True)(x)\n",
    "#     x = layers.Dropout(0.3)(x)\n",
    "#     x = layers.Dense(128, activation = 'relu', kernel_initializer = 'he_normal')(x)\n",
    "#     x = layers.BatchNormalization(center=True, scale=True)(x)\n",
    "#     x = layers.Dropout(0.3)(x)\n",
    "#     out_ = layers.Dense(output_shape, activation = activation, use_bias = False)(x) # activation = linear!\n",
    "#     nn_im = keras.Model(inputs = in_, outputs = out_)\n",
    "#     return nn_im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONTRAMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for all models\n",
    "C = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Intercept, Complex Shift: TMAX\n",
    "Learn first to predict the outcome with TMAX images alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"SI_CSb_TMAX/\"\n",
    "\n",
    "## create folders: run only once\n",
    "#os.mkdir(OUTPUT_DIR)\n",
    "#os.mkdir(OUTPUT_DIR + folder_name)\n",
    "#for i in range(5):\n",
    "#    os.mkdir(OUTPUT_DIR + folder_name + \"fold\" + str(i))\n",
    "#    for j in range(5):\n",
    "#        os.mkdir(OUTPUT_DIR + folder_name + \"fold\" + str(i) + \"/run\" + str(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 8\n",
    "toplayer_epochs = 40\n",
    "alllayer_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depends on the model\n",
    "def train_preprocessing(data, label):\n",
    "    \"\"\"Process training data.\"\"\"\n",
    "    intercept = data[0] # intercept\n",
    "    volume = data[1] # shift: image\n",
    "    volume = zoom(volume)\n",
    "    volume = rotate(volume)\n",
    "    volume = shift(volume)\n",
    "    volume = flip(volume)\n",
    "    return (intercept, volume), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "j = 0\n",
    "my_seed = 1\n",
    "nll = np.empty((5, 1))\n",
    "\n",
    "# for train_idx, test_idx in kf.split(X): # folds\n",
    "for f in range(len(train_idxes)):\n",
    "    \n",
    "    train_idx = train_idxes[f]\n",
    "    test_idx = test_idxes[f]\n",
    "    \n",
    "    # Load data for fold j ------------------------------------------------------\n",
    "    \n",
    "    X_train = X_tmax[train_idx-1]\n",
    "    X_valid = X_tmax[valid_idx-1]\n",
    "    X_test = X_tmax[test_idx-1]\n",
    "    \n",
    "    Y_train = Y[train_idx-1]\n",
    "    Y_valid = Y[valid_idx-1]\n",
    "    Y_test = Y[test_idx-1]\n",
    "    \n",
    "    dat_train = train[f]\n",
    "    dat_valid = valid[f]\n",
    "    dat_test = test[f]\n",
    "    \n",
    "    dat_train.to_csv(OUTPUT_DIR +  folder_name + \"fold\" + str(j) + \"/\" + \"dat_train.csv\", index = False)\n",
    "    dat_valid.to_csv(OUTPUT_DIR +  folder_name + \"fold\" + str(j) + \"/\" + \"dat_valid.csv\", index = False)\n",
    "    dat_test.to_csv(OUTPUT_DIR +  folder_name + \"fold\" + str(j) + \"/\" + \"dat_test.csv\", index = False)    \n",
    "    \n",
    "    \n",
    "    # Define datasets for ONTRAM ------------------------------------------------------\n",
    "\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((np.ones(shape=[len(X_train),1]), X_train))\n",
    "    train_labels = tf.data.Dataset.from_tensor_slices((Y_train))\n",
    "    \n",
    "    valid_data = tf.data.Dataset.from_tensor_slices((np.ones(shape=[len(X_valid),1]), X_valid))\n",
    "    valid_labels = tf.data.Dataset.from_tensor_slices((Y_valid))\n",
    "    \n",
    "    test_data = tf.data.Dataset.from_tensor_slices((np.ones(shape=[len(X_test),1]), X_test))\n",
    "    test_labels = tf.data.Dataset.from_tensor_slices((Y_test))\n",
    "    \n",
    "    train_loader = tf.data.Dataset.zip((train_data, train_labels))\n",
    "    validation_loader = tf.data.Dataset.zip((valid_data, valid_labels))\n",
    "    test_loader = tf.data.Dataset.zip((test_data, test_labels))\n",
    "    \n",
    "    train_dataset = (train_loader.shuffle(len(X_train))\n",
    "                     .map(train_preprocessing)\n",
    "                     .batch(batch_size, drop_remainder = True))\n",
    "    validation_dataset = (validation_loader.batch(batch_size, drop_remainder = True))\n",
    "    test_dataset = (test_loader.batch(len(X_test)))\n",
    "    \n",
    "    \n",
    "    # Training ---------------------------------------------------------------------\n",
    "    \n",
    "    for i in range(5):\n",
    "        \n",
    "        # Define model\n",
    "        mbl = mod_baseline(C)\n",
    "        mcs = img_model(1, \"linear\")\n",
    "        \n",
    "        # start to train the top layers\n",
    "        for layer in mcs.layers:\n",
    "            layer.trainable = False\n",
    "        model = ontram(mbl, mcs)\n",
    "        \n",
    "        # save weights of the best model\n",
    "        checkpoint_filepath = OUTPUT_DIR + folder_name + \"/fold\" + str(j) + \"/\" + \"run\" + str(i) + \"/\"\n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_filepath,\n",
    "                                                                       save_weights_only = True,\n",
    "                                                                       monitor = \"val_loss\",\n",
    "                                                                       mode = \"min\",\n",
    "                                                                       save_best_only = True)\n",
    "        \n",
    "        # compile and train\n",
    "        model.compile(optimizer = keras.optimizers.Adam(),\n",
    "                    loss = ontram_loss(C, batch_size),\n",
    "                    metrics = [ontram_acc(C, batch_size)])\n",
    "        history_0 = model.fit(train_dataset,\n",
    "                              validation_data = validation_dataset,\n",
    "                              epochs = toplayer_epochs,\n",
    "                              shuffle = True,\n",
    "                              verbose = 2,\n",
    "                              callbacks=[model_checkpoint_callback])\n",
    "        \n",
    "        # Load the wieghts and unfreeze the layers\n",
    "        model.load_weights(checkpoint_filepath)\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = True\n",
    "        \n",
    "        # recompile the model for the modifications to take effect\n",
    "        model.compile(optimizer = keras.optimizers.Adam(lr=0.0001), \n",
    "                      loss = ontram_loss(C, batch_size),\n",
    "                      metrics = [ontram_acc(C, batch_size)])   \n",
    "        history_1 = model.fit(train_dataset,\n",
    "                              validation_data = validation_dataset,\n",
    "                              epochs = alllayer_epochs,\n",
    "                              shuffle = True,\n",
    "                              verbose = 2,\n",
    "                              callbacks=[model_checkpoint_callback])\n",
    "        \n",
    "        # save the history\n",
    "        pd.DataFrame(history_0.history).append(pd.DataFrame(history_1.history)).to_csv(checkpoint_filepath + \"history.csv\", index = False)\n",
    "        \n",
    "        # Do predictions\n",
    "        model.load_weights(checkpoint_filepath)\n",
    "        test_batch_size = len(X_test)\n",
    "        model.compile(loss = ontram_loss(C, test_batch_size))\n",
    "        preds = predict_ontram(model, data = test_dataset)\n",
    "        \n",
    "        # save predictions/parameters\n",
    "        cdf = pd.DataFrame(preds[\"cdf\"])\n",
    "        cdf.columns = [\"y_pred\" + str(i) for i in range(8)]\n",
    "        cdf[\"p_id\"] = dat_test.p_id.values\n",
    "        cdf[\"y_true\"] = np.argmax(Y_test, axis=1)\n",
    "        cdf.to_csv(checkpoint_filepath + \"cdf.csv\", index = False) \n",
    "        \n",
    "        pdf = pd.DataFrame(preds[\"pdf\"])\n",
    "        pdf.columns = [\"y_pred\" + str(i) for i in range(7)]\n",
    "        pdf[\"p_id\"] = dat_test.p_id.values\n",
    "        pdf[\"y_true\"] = np.argmax(Y_test, axis=1)\n",
    "        pdf.to_csv(checkpoint_filepath + \"pdf.csv\", index = False)\n",
    "        \n",
    "        nll[i] = preds[\"nll\"]\n",
    "    \n",
    "    pd.DataFrame(nll).to_csv(OUTPUT_DIR + folder_name + \"/fold\" + str(j) + \"/nll.csv\", index = False)\n",
    "    \n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Intercept, Complex Shift: TMAX + DWI\n",
    "Learn to combine the pretained networks for TMAX and DWI images\n",
    "- Load both models\n",
    "- Remove the FC part for both models\n",
    "- Learn a new FC part with weights fixed for the convolutional parts\n",
    "- Fine-tune the whole model with smaller learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"SI_CSb_TMAX_DWI/\"\n",
    "\n",
    "# # create folders: run only once\n",
    "# #os.mkdir(OUTPUT_DIR)\n",
    "# os.mkdir(OUTPUT_DIR + folder_name)\n",
    "# for i in range(5):\n",
    "#     os.mkdir(OUTPUT_DIR + folder_name + \"fold\" + str(i))\n",
    "#     for j in range(5):\n",
    "#         os.mkdir(OUTPUT_DIR + folder_name + \"fold\" + str(i) + \"/run\" + str(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 8\n",
    "toplayer_epochs = 40\n",
    "alllayer_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_img_model(nout, dir_dwi, dir_tmax, fix_conv_parts = True, last_layer_activation = \"linear\"):\n",
    "    in_ = keras.Input(shape = (128, 128, 20, 3, 2))\n",
    "    # split the input\n",
    "    dwi_in_ = layers.Lambda(lambda x: x[:,:,:,:,:,0])(in_) #(None, 128, 128, 20, 3, 2)\n",
    "    tmax_in_ = layers.Lambda(lambda x: x[:,:,:,:,:,1])(in_)\n",
    "    \n",
    "    # define dwi model and load weights\n",
    "    mbl = mod_baseline(C)\n",
    "    mcs = img_model(1, \"linear\")\n",
    "    ontram_dwi = ontram(mbl, mcs)\n",
    "    ontram_dwi.load_weights(dir_dwi).expect_partial()\n",
    "    conv_part_dwi = keras.Model(ontram_dwi.mod_shift[0].input, \n",
    "                                ontram_dwi.mod_shift[0].layers[-5].output) # global average pooling layer (output of conv part)\n",
    "  \n",
    "    # define tmax model and load weights\n",
    "    mbl = mod_baseline(C)\n",
    "    mcs = img_model(1, \"linear\")\n",
    "    ontram_tmax = ontram(mbl, mcs)\n",
    "    ontram_tmax.load_weights(dir_tmax).expect_partial()\n",
    "    conv_part_tmax = keras.Model(ontram_tmax.mod_shift[0].input, \n",
    "                                 ontram_tmax.mod_shift[0].layers[-5].output) # global average pooling layer (output of conv part)\n",
    "    \n",
    "    # fix convolutional parts\n",
    "    if(fix_conv_parts):\n",
    "        for layer in conv_part_dwi.layers:\n",
    "            layer.trainable = False\n",
    "        for layer in conv_part_tmax.layers:\n",
    "            layer.trainable = False\n",
    "        \n",
    "    x_dwi = conv_part_dwi(dwi_in_)\n",
    "    x_tmax = conv_part_tmax(tmax_in_)\n",
    "    x = keras.layers.concatenate((x_dwi, x_tmax))\n",
    "    x = keras.layers.Dense(128, name=\"fc1\")(x)\n",
    "    x = keras.layers.Dense(128, name=\"fc2\")(x)\n",
    "    x = keras.layers.Dense(128, name=\"fc3\")(x)\n",
    "    x = keras.layers.Dense(nout, name=\"output\")(x)\n",
    "    predictions = keras.layers.Activation(last_layer_activation, name='output_activation')(x)\n",
    "    \n",
    "    return keras.Model(inputs=in_, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_filepath_dwi = OUTPUT_DIR_DWI + \"SI_CSb/fold\" + str(j) + \"/\" + \"run\" + str(i) + \"/\"\n",
    "# checkpoint_filepath_tmax = OUTPUT_DIR + \"SI_CSb_TMAX/fold\" + str(j) + \"/\" + \"run\" + str(i) + \"/\"\n",
    "# \n",
    "# model = combined_img_model(C, checkpoint_filepath_dwi, checkpoint_filepath_tmax, \n",
    "#                         fix_conv_parts = True, last_layer_activation = \"linear\")\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depends on the model\n",
    "def train_preprocessing(data, label):\n",
    "    \"\"\"Process training data.\"\"\"\n",
    "    intercept = data[0] # intercept\n",
    "    volume = data[1] # shift: image\n",
    "    #volume = zoom2(volume)\n",
    "    volume = rotate2(volume)\n",
    "    volume = shift2(volume)\n",
    "    volume = flip2(volume)\n",
    "    return (intercept, volume), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "j = 3\n",
    "my_seed = 1\n",
    "nll = np.empty((5, 1))\n",
    "\n",
    "# for train_idx, test_idx in kf.split(X): # folds\n",
    "#for f in range(len(train_idxes)):\n",
    "for f in range(3,5):\n",
    "    \n",
    "    train_idx = train_idxes[f]\n",
    "    test_idx = test_idxes[f]\n",
    "    \n",
    "    # Load data for fold j ------------------------------------------------------\n",
    "    \n",
    "    X_train = X[train_idx-1]\n",
    "    X_valid = X[valid_idx-1]\n",
    "    X_test = X[test_idx-1]\n",
    "    \n",
    "    Y_train = Y[train_idx-1]\n",
    "    Y_valid = Y[valid_idx-1]\n",
    "    Y_test = Y[test_idx-1]\n",
    "    \n",
    "    dat_train = train[f]\n",
    "    dat_valid = valid[f]\n",
    "    dat_test = test[f]\n",
    "    \n",
    "    dat_train.to_csv(OUTPUT_DIR +  folder_name + \"fold\" + str(j) + \"/\" + \"dat_train.csv\", index = False)\n",
    "    dat_valid.to_csv(OUTPUT_DIR +  folder_name + \"fold\" + str(j) + \"/\" + \"dat_valid.csv\", index = False)\n",
    "    dat_test.to_csv(OUTPUT_DIR +  folder_name + \"fold\" + str(j) + \"/\" + \"dat_test.csv\", index = False)    \n",
    "    \n",
    "    \n",
    "    # Define datasets for ONTRAM ------------------------------------------------------\n",
    "\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((np.ones(shape=[len(X_train),1]), X_train))\n",
    "    train_labels = tf.data.Dataset.from_tensor_slices((Y_train))\n",
    "    \n",
    "    valid_data = tf.data.Dataset.from_tensor_slices((np.ones(shape=[len(X_valid),1]), X_valid))\n",
    "    valid_labels = tf.data.Dataset.from_tensor_slices((Y_valid))\n",
    "    \n",
    "    test_data = tf.data.Dataset.from_tensor_slices((np.ones(shape=[len(X_test),1]), X_test))\n",
    "    test_labels = tf.data.Dataset.from_tensor_slices((Y_test))\n",
    "    \n",
    "    train_loader = tf.data.Dataset.zip((train_data, train_labels))\n",
    "    validation_loader = tf.data.Dataset.zip((valid_data, valid_labels))\n",
    "    test_loader = tf.data.Dataset.zip((test_data, test_labels))\n",
    "    \n",
    "    train_dataset = (train_loader.shuffle(len(X_train))\n",
    "                     .map(train_preprocessing)\n",
    "                     .batch(batch_size, drop_remainder = True))\n",
    "    validation_dataset = (validation_loader.batch(batch_size, drop_remainder = True))\n",
    "    test_dataset = (test_loader.batch(len(X_test)))\n",
    "    \n",
    "    \n",
    "    # Training ---------------------------------------------------------------------\n",
    "    \n",
    "    for i in range(0,2):\n",
    "        \n",
    "        # Define combined model for DWI & TMAX\n",
    "        checkpoint_filepath_dwi = OUTPUT_DIR_DWI + \"SI_CSb/fold\" + str(j) + \"/\" + \"run\" + str(i) + \"/\"\n",
    "        checkpoint_filepath_tmax = OUTPUT_DIR + \"SI_CSb_TMAX/fold\" + str(j) + \"/\" + \"run\" + str(i) + \"/\"\n",
    "        mcs_combined = combined_img_model(1, checkpoint_filepath_dwi, checkpoint_filepath_tmax, \n",
    "                                          fix_conv_parts = True, last_layer_activation = \"linear\")\n",
    "        \n",
    "        # take the weights of the DWI baseline model\n",
    "        mbl = mod_baseline(C)\n",
    "        mcs = img_model(1, \"linear\")\n",
    "        ontram_dwi = ontram(mbl, mcs)\n",
    "        ontram_dwi.load_weights(checkpoint_filepath_dwi).expect_partial()\n",
    "\n",
    "        # ontram model\n",
    "        model = ontram(ontram_dwi.mod_baseline, mcs_combined)\n",
    "        \n",
    "        # save weights of the best model\n",
    "        checkpoint_filepath = OUTPUT_DIR + folder_name + \"/fold\" + str(j) + \"/\" + \"run\" + str(i) + \"/\"\n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_filepath,\n",
    "                                                                       save_weights_only = True,\n",
    "                                                                       monitor = \"val_loss\",\n",
    "                                                                       mode = \"min\",\n",
    "                                                                       save_best_only = True)\n",
    "        \n",
    "        # compile and train\n",
    "        model.compile(optimizer = keras.optimizers.Adam(),\n",
    "                    loss = ontram_loss(C, batch_size),\n",
    "                    metrics = [ontram_acc(C, batch_size)])\n",
    "        history_0 = model.fit(train_dataset,\n",
    "                              validation_data = validation_dataset,\n",
    "                              epochs = toplayer_epochs,\n",
    "                              shuffle = True,\n",
    "                              verbose = 2,\n",
    "                              callbacks=[model_checkpoint_callback])\n",
    "        \n",
    "        # Load the wieghts and unfreeze the layers\n",
    "        model.load_weights(checkpoint_filepath)\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = True\n",
    "        \n",
    "        # recompile the model for the modifications to take effect\n",
    "        model.compile(optimizer = keras.optimizers.Adam(lr=0.0001), \n",
    "                      loss = ontram_loss(C, batch_size),\n",
    "                      metrics = [ontram_acc(C, batch_size)])   \n",
    "        history_1 = model.fit(train_dataset,\n",
    "                              validation_data = validation_dataset,\n",
    "                              epochs = alllayer_epochs,\n",
    "                              shuffle = True,\n",
    "                              verbose = 2,\n",
    "                              callbacks=[model_checkpoint_callback])\n",
    "        \n",
    "        # save the history\n",
    "        pd.DataFrame(history_0.history).append(pd.DataFrame(history_1.history)).to_csv(checkpoint_filepath + \"history.csv\", index = False)\n",
    "        \n",
    "        # Do predictions\n",
    "        model.load_weights(checkpoint_filepath)\n",
    "        test_batch_size = len(X_test)\n",
    "        model.compile(loss = ontram_loss(C, test_batch_size))\n",
    "        preds = predict_ontram(model, data = test_dataset)\n",
    "        \n",
    "        # save predictions/parameters\n",
    "        cdf = pd.DataFrame(preds[\"cdf\"])\n",
    "        cdf.columns = [\"y_pred\" + str(i) for i in range(8)]\n",
    "        cdf[\"p_id\"] = dat_test.p_id.values\n",
    "        cdf[\"y_true\"] = np.argmax(Y_test, axis=1)\n",
    "        cdf.to_csv(checkpoint_filepath + \"cdf.csv\", index = False) \n",
    "        \n",
    "        pdf = pd.DataFrame(preds[\"pdf\"])\n",
    "        pdf.columns = [\"y_pred\" + str(i) for i in range(7)]\n",
    "        pdf[\"p_id\"] = dat_test.p_id.values\n",
    "        pdf[\"y_true\"] = np.argmax(Y_test, axis=1)\n",
    "        pdf.to_csv(checkpoint_filepath + \"pdf.csv\", index = False)\n",
    "        \n",
    "        nll[i] = preds[\"nll\"]\n",
    "    \n",
    "    pd.DataFrame(nll).to_csv(OUTPUT_DIR + folder_name + \"/fold\" + str(j) + \"/nll.csv\", index = False)\n",
    "    \n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple intercept, Complex shift, Linear shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_name = \"SI_LSx_CSb_TMAX_DWI/\"\n",
    "# IMG_MODEL_DIR = \"SI_CSb_TMAX_DWI/\"\n",
    "# POLR_DIR = OUTPUT_DIR_DWI + \"SI_LSx/\"\n",
    "# \n",
    "# ## create folders: run only once\n",
    "# #os.mkdir(OUTPUT_DIR + folder_name)\n",
    "# #for i in range(5):\n",
    "# #    os.mkdir(OUTPUT_DIR + folder_name + \"fold\" + str(i))\n",
    "# #    for j in range(5):\n",
    "# #        os.mkdir(OUTPUT_DIR + folder_name + \"fold\" + str(i) + \"/run\" + str(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters\n",
    "# batch_size = 8\n",
    "# toplayer_epochs = 40\n",
    "# alllayer_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # function for preprocessing\n",
    "# def train_preprocessing(data, label):\n",
    "#     \"\"\"Process training data.\"\"\"\n",
    "#     intercept = data[0] # intercept\n",
    "#     volume = data[1] # shift: image\n",
    "#     tabular = data[2] # shift: tabular\n",
    "#     volume = zoom(volume)\n",
    "#     volume = rotate(volume)\n",
    "#     volume = shift(volume)\n",
    "#     volume = flip(volume)\n",
    "#     return (intercept, volume, tabular), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# j = 0\n",
    "# my_seed = 1\n",
    "# nll = np.empty((5, 1))\n",
    "# estimates = np.empty((5, 6+9))\n",
    "# estimates_sd = np.empty((5, 6+9))\n",
    "# \n",
    "# # for train_idx, test_idx in kf.split(X): # folds\n",
    "# for f in range(len(train_idxes)):\n",
    "#     \n",
    "#     train_idx = train_idxes[f]\n",
    "#     test_idx = test_idxes[f]\n",
    "#     \n",
    "#     # Load data for fold j ------------------------------------------------------\n",
    "#     \n",
    "#     # define datasets\n",
    "#     X_tab_train = np.array([train[f].age, train[f].nihss_bl, train[f].sys_bloodpressure_bl, \n",
    "#                             train[f].rf_diabetes, train[f].rf_hypertonia, train[f].rf_smoker, \n",
    "#                             train[f].rf_tia_stroke, train[f].lyse, train[f].time_to_groin_puncture]).T\n",
    "#     X_tab_valid = np.array([valid[f].age, valid[f].nihss_bl, valid[f].sys_bloodpressure_bl, \n",
    "#                             valid[f].rf_diabetes, valid[f].rf_hypertonia, valid[f].rf_smoker, \n",
    "#                             valid[f].rf_tia_stroke, valid[f].lyse, valid[f].time_to_groin_puncture]).T\n",
    "#     X_tab_test = np.array([test[f].age, test[f].nihss_bl, test[f].sys_bloodpressure_bl, \n",
    "#                             test[f].rf_diabetes, test[f].rf_hypertonia, test[f].rf_smoker, \n",
    "#                             test[f].rf_tia_stroke, test[f].lyse, test[f].time_to_groin_puncture]).T\n",
    "#     # normalize\n",
    "#     sd = np.empty((9,))\n",
    "#     for i in range(X_tab_train.shape[1]):\n",
    "#         train_mean = np.mean(X_tab_train[:,i])\n",
    "#         train_std = np.std(X_tab_train[:,i])\n",
    "#         X_tab_train[:,i] = (X_tab_train[:,i] - train_mean) / train_std\n",
    "#         X_tab_valid[:,i] = (X_tab_valid[:,i] - train_mean) / train_std\n",
    "#         X_tab_test[:,i] = (X_tab_test[:,i] - train_mean) / train_std\n",
    "#         sd[i] = train_std\n",
    "#     \n",
    "#     X_train = X[train_idx-1]\n",
    "#     X_valid = X[valid_idx-1]\n",
    "#     X_test = X[test_idx-1]\n",
    "#     \n",
    "#     Y_train = Y[train_idx-1]\n",
    "#     Y_valid = Y[valid_idx-1]\n",
    "#     Y_test = Y[test_idx-1]\n",
    "#     \n",
    "#     dat_train = train[f]\n",
    "#     dat_valid = valid[f]\n",
    "#     dat_test = test[f]\n",
    "#     \n",
    "#     dat_train.to_csv(OUTPUT_DIR +  folder_name + \"fold\" + str(j) + \"/\" + \"dat_train.csv\", index = False)\n",
    "#     dat_valid.to_csv(OUTPUT_DIR +  folder_name + \"fold\" + str(j) + \"/\" + \"dat_valid.csv\", index = False)\n",
    "#     dat_test.to_csv(OUTPUT_DIR +  folder_name + \"fold\" + str(j) + \"/\" + \"dat_test.csv\", index = False)  \n",
    "#     \n",
    "#     \n",
    "#     # Define datasets for ONTRAM ------------------------------------------------------\n",
    "#     \n",
    "#     train_data = tf.data.Dataset.from_tensor_slices((np.ones(shape=[len(X_train),1]), X_train, X_tab_train))\n",
    "#     train_labels = tf.data.Dataset.from_tensor_slices((Y_train))\n",
    "#     \n",
    "#     valid_data = tf.data.Dataset.from_tensor_slices((np.ones(shape=[len(X_valid),1]), X_valid, X_tab_valid))\n",
    "#     valid_labels = tf.data.Dataset.from_tensor_slices((Y_valid))\n",
    "#     \n",
    "#     test_data = tf.data.Dataset.from_tensor_slices((np.ones(shape=[len(X_test),1]), X_test, X_tab_test))\n",
    "#     test_labels = tf.data.Dataset.from_tensor_slices((Y_test))\n",
    "#     \n",
    "#     train_loader = tf.data.Dataset.zip((train_data, train_labels))\n",
    "#     validation_loader = tf.data.Dataset.zip((valid_data, valid_labels))\n",
    "#     test_loader = tf.data.Dataset.zip((test_data, test_labels))\n",
    "#     \n",
    "#     train_dataset = (train_loader.shuffle(len(X_train))\n",
    "#                      .map(train_preprocessing)\n",
    "#                      .batch(batch_size, drop_remainder = True))\n",
    "#     validation_dataset = (validation_loader.batch(batch_size, drop_remainder = True))\n",
    "#     test_dataset = (test_loader.batch(len(X_test), drop_remainder = True))\n",
    "# \n",
    "#     # Training ---------------------------------------------------------------------\n",
    "#     \n",
    "#     for i in range(5):\n",
    "#         \n",
    "#         # folder to save weights \n",
    "# \n",
    "#         # define model and load weights from SI_LSx\n",
    "#         mbl = mod_baseline(C)\n",
    "#         mls = mod_linear_shift(X_tab_train.shape[1])\n",
    "#         polr = ontram(mbl, mls)\n",
    "#         polr.load_weights(POLR_DIR + \"fold\" + str(j) + \"/run\" + str(i) + \"/model_weights.hdf5\")\n",
    "#         mcs = img_model(1, \"linear\")\n",
    "#         \n",
    "#         # start to train the top layers\n",
    "#         for layer in mcs.layers:\n",
    "#             layer.trainable = False\n",
    "#         model = ontram(polr.mod_baseline, [mcs, polr.mod_shift[0]])\n",
    "#         \n",
    "#         # save weights of the best model\n",
    "#         checkpoint_filepath = OUTPUT_DIR + folder_name + \"/fold\" + str(j) + \"/\" + \"run\" + str(i) + \"/\"\n",
    "#         model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_filepath,\n",
    "#                                                                        save_weights_only = True,\n",
    "#                                                                        monitor = \"val_loss\",\n",
    "#                                                                        mode = \"min\",\n",
    "#                                                                        save_best_only = True)\n",
    "#         \n",
    "#         # compile and train\n",
    "#         model.compile(optimizer = keras.optimizers.Adam(),\n",
    "#                     loss = ontram_loss(C, batch_size),\n",
    "#                     metrics = [ontram_acc(C, batch_size)])\n",
    "#         history_0 = model.fit(train_dataset,\n",
    "#                               validation_data = validation_dataset,\n",
    "#                               epochs = toplayer_epochs,\n",
    "#                               shuffle = True,\n",
    "#                               verbose = 2,\n",
    "#                               callbacks=[model_checkpoint_callback])\n",
    "#         \n",
    "#         # Load the wieghts and unfreeze the layers\n",
    "#         model.load_weights(checkpoint_filepath)\n",
    "#         for layer in model.layers:\n",
    "#             layer.trainable = True\n",
    "#         \n",
    "#         # recompile the model for the modifications to take effect\n",
    "#         model.compile(optimizer = keras.optimizers.Adam(lr=0.0001), \n",
    "#                       loss = ontram_loss(C, batch_size),\n",
    "#                       metrics = [ontram_acc(C, batch_size)])   \n",
    "#         history_1 = model.fit(train_dataset,\n",
    "#                               validation_data = validation_dataset,\n",
    "#                               epochs = alllayer_epochs,\n",
    "#                               shuffle = True,\n",
    "#                               verbose = 2,\n",
    "#                               callbacks=[model_checkpoint_callback])\n",
    "#         \n",
    "#         # save the history\n",
    "#         pd.DataFrame(history_0.history).append(pd.DataFrame(history_1.history)).to_csv(checkpoint_filepath + \"history.csv\", index = False)\n",
    "#         \n",
    "#         # Do predictions\n",
    "#         model.load_weights(checkpoint_filepath)\n",
    "#         test_batch_size = len(X_test)\n",
    "#         model.compile(loss = ontram_loss(C, test_batch_size))\n",
    "#         preds = predict_ontram(model, data = test_dataset)\n",
    "#         params = get_parameters(model)\n",
    "#         \n",
    "#         # save predictions/parameters\n",
    "#         cdf = pd.DataFrame(preds[\"cdf\"])\n",
    "#         cdf.columns = [\"y_pred\" + str(i) for i in range(8)]\n",
    "#         cdf[\"p_id\"] = dat_test.p_id.values\n",
    "#         cdf[\"y_true\"] = np.argmax(Y_test, axis=1)\n",
    "#         cdf.to_csv(checkpoint_filepath + \"cdf.csv\", index = False)  \n",
    "#         \n",
    "#         pdf = pd.DataFrame(preds[\"pdf\"])\n",
    "#         pdf.columns = [\"y_pred\" + str(i) for i in range(7)]\n",
    "#         pdf[\"p_id\"] = dat_test.p_id.values\n",
    "#         pdf[\"y_true\"] = np.argmax(Y_test, axis=1)\n",
    "#         pdf.to_csv(checkpoint_filepath + \"pdf.csv\", index = False)\n",
    "#         \n",
    "#         nll[i] = preds[\"nll\"]\n",
    "#         estimates[i] = np.concatenate((params[\"intercept\"][0][0][0], np.concatenate(params[\"shift\"][0][1][0]))) \n",
    "#         estimates_sd[i] = np.concatenate((params[\"intercept\"][0][0][0], np.concatenate(params[\"shift\"][0][1][0])/sd)) \n",
    "#     \n",
    "#     pd.DataFrame(nll).to_csv(OUTPUT_DIR + folder_name + \"/fold\" + str(j) + \"/nll.csv\", index = False)\n",
    "#     \n",
    "#     pd_estimates = pd.DataFrame(estimates)\n",
    "#     pd_estimates.columns = [\"intercept0\", \"intercept1\", \"intercept2\", \"intercept3\", \"intercept4\", \n",
    "#                             \"intercept5\", \"age\", \"nihss_bl\", \"sys_bloodpressure_bl\", \"rf_diabetes\", \n",
    "#                             \"rf_hypertonia\", \"rf_smoker\", \"rf_tia_stroke\", \"lyse\", \"time_to_groin_puncture\"]\n",
    "#     pd_estimates.to_csv(OUTPUT_DIR + folder_name + \"/fold\" + str(j) + \"/estimates.csv\", index = False)\n",
    "#     \n",
    "#     pd_estimates_sd = pd.DataFrame(estimates_sd)\n",
    "#     pd_estimates_sd.columns = [\"intercept0\", \"intercept1\", \"intercept2\", \"intercept3\", \"intercept4\", \n",
    "#                               \"intercept5\", \"age\", \"nihss_bl\", \"sys_bloodpressure_bl\", \"rf_diabetes\", \n",
    "#                               \"rf_hypertonia\", \"rf_smoker\", \"rf_tia_stroke\", \"lyse\", \"time_to_groin_puncture\"]\n",
    "#     pd_estimates_sd.to_csv(OUTPUT_DIR + folder_name + \"/fold\" + str(j) + \"/estimates_sd.csv\", index = False)\n",
    "#     \n",
    "#     j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trafo ensembles\n",
    "Calculate the average CDF per patient by averaging the transformation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_names = [\"SI_CSb_TMAX\", \"SI_CSb_TMAX_DWI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder_name in folder_names:\n",
    "    for f in range(5): # fold\n",
    "        checkpoint_filepath = OUTPUT_DIR + folder_name + \"/fold\" + str(f) + \"/\"\n",
    "        cdf0 = pd.read_csv(checkpoint_filepath + \"run0/cdf.csv\")\n",
    "        cdf1 = pd.read_csv(checkpoint_filepath + \"run1/cdf.csv\")\n",
    "        cdf2 = pd.read_csv(checkpoint_filepath + \"run2/cdf.csv\")\n",
    "        cdf3 = pd.read_csv(checkpoint_filepath + \"run3/cdf.csv\")\n",
    "        cdf4 = pd.read_csv(checkpoint_filepath + \"run4/cdf.csv\")\n",
    "        cdf = pd.concat((cdf0[[\"y_pred0\", \"y_pred1\", \"y_pred2\", \"y_pred3\", \"y_pred4\", \"y_pred5\", \"y_pred6\", \"y_pred7\"]], \n",
    "                         cdf1[[\"y_pred0\", \"y_pred1\", \"y_pred2\", \"y_pred3\", \"y_pred4\", \"y_pred5\", \"y_pred6\", \"y_pred7\"]], \n",
    "                         cdf2[[\"y_pred0\", \"y_pred1\", \"y_pred2\", \"y_pred3\", \"y_pred4\", \"y_pred5\", \"y_pred6\", \"y_pred7\"]], \n",
    "                         cdf3[[\"y_pred0\", \"y_pred1\", \"y_pred2\", \"y_pred3\", \"y_pred4\", \"y_pred5\", \"y_pred6\", \"y_pred7\"]], \n",
    "                         cdf4[[\"y_pred0\", \"y_pred1\", \"y_pred2\", \"y_pred3\", \"y_pred4\", \"y_pred5\", \"y_pred6\", \"y_pred7\"]]))\n",
    "        by_row_index = cdf.groupby(cdf.index)\n",
    "        \n",
    "        trafo_cdf = by_row_index.apply(lambda x: expit(np.mean(logit(x))))\n",
    "        trafo_pdf = np.array(trafo_cdf)[:,1:] - np.array(trafo_cdf)[:,:-1]\n",
    "        y_pred = np.argmax(trafo_pdf, axis = 1)\n",
    "        y_pred_fav = np.sum(trafo_pdf[:,:3], axis = 1) # mRS 0-2\n",
    "        y_pred_unfav = np.sum(trafo_pdf[:,3:], axis = 1) # mRS 3-6\n",
    "        y_pred_bin = np.where(y_pred_fav>0.5, 1, 0)\n",
    "        y_true_bin = np.where(cdf0.y_true<=2, 1, 0)\n",
    "        \n",
    "        trafo_cdf[\"p_id\"] = cdf0.p_id\n",
    "        trafo_cdf[\"y_true\"] = cdf0.y_true\n",
    "        trafo_cdf[\"y_pred\"] = y_pred\n",
    "        trafo_cdf.to_csv(checkpoint_filepath + \"/trafo_cdf.csv\", index = False)\n",
    "        \n",
    "        trafo_pdf = pd.DataFrame(trafo_pdf)\n",
    "        trafo_pdf.columns = [\"y_pred\" + str(i) for i in range(7)]\n",
    "        trafo_pdf[\"p_id\"] = cdf0.p_id\n",
    "        trafo_pdf[\"y_true\"] = cdf0.y_true \n",
    "        trafo_pdf[\"y_pred\"] = y_pred\n",
    "        trafo_pdf.to_csv(checkpoint_filepath + \"/trafo_pdf.csv\", index = False)\n",
    "        \n",
    "        trafo_pdf_bin = pd.DataFrame({\"y_pred1_fav\": y_pred_fav, \n",
    "                                      \"y_pred0_unfav\": y_pred_unfav,\n",
    "                                      \"y_pred\": y_pred_bin,\n",
    "                                      \"y_true\": y_true_bin,\n",
    "                                      \"p_id\": cdf0.p_id})\n",
    "        trafo_pdf_bin.to_csv(checkpoint_filepath + \"/trafo_pdf_bin.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all test samples\n",
    "for folder_name in folder_names:\n",
    "    checkpoint_filepath = OUTPUT_DIR + folder_name \n",
    "    cdf0 = pd.read_csv(checkpoint_filepath + \"/fold0/trafo_cdf.csv\")\n",
    "    cdf1 = pd.read_csv(checkpoint_filepath + \"/fold1/trafo_cdf.csv\")\n",
    "    cdf2 = pd.read_csv(checkpoint_filepath + \"/fold2/trafo_cdf.csv\")\n",
    "    cdf3 = pd.read_csv(checkpoint_filepath + \"/fold3/trafo_cdf.csv\")\n",
    "    cdf4 = pd.read_csv(checkpoint_filepath + \"/fold4/trafo_cdf.csv\")\n",
    "    cdf = pd.concat((cdf0, cdf1, cdf2, cdf3, cdf4))\n",
    "    cdf.to_csv(OUTPUT_DIR + folder_name + \"/test_cdf.csv\", index = False)\n",
    "    \n",
    "    pdf0 = pd.read_csv(checkpoint_filepath + \"/fold0/trafo_pdf.csv\")\n",
    "    pdf1 = pd.read_csv(checkpoint_filepath + \"/fold1/trafo_pdf.csv\")\n",
    "    pdf2 = pd.read_csv(checkpoint_filepath + \"/fold2/trafo_pdf.csv\")\n",
    "    pdf3 = pd.read_csv(checkpoint_filepath + \"/fold3/trafo_pdf.csv\")\n",
    "    pdf4 = pd.read_csv(checkpoint_filepath + \"/fold4/trafo_pdf.csv\")\n",
    "    pdf = pd.concat((pdf0, pdf1, pdf2, pdf3, pdf4))\n",
    "    pdf.to_csv(OUTPUT_DIR + folder_name + \"/test_pdf.csv\", index = False)\n",
    "    \n",
    "    pdf_bin0 = pd.read_csv(checkpoint_filepath + \"/fold0/trafo_pdf_bin.csv\")\n",
    "    pdf_bin1 = pd.read_csv(checkpoint_filepath + \"/fold1/trafo_pdf_bin.csv\")\n",
    "    pdf_bin2 = pd.read_csv(checkpoint_filepath + \"/fold2/trafo_pdf_bin.csv\")\n",
    "    pdf_bin3 = pd.read_csv(checkpoint_filepath + \"/fold3/trafo_pdf_bin.csv\")\n",
    "    pdf_bin4 = pd.read_csv(checkpoint_filepath + \"/fold4/trafo_pdf_bin.csv\")\n",
    "    pdf_bin = pd.concat((pdf_bin0, pdf_bin1, pdf_bin2, pdf_bin3, pdf_bin4))\n",
    "    pdf_bin.to_csv(OUTPUT_DIR + folder_name + \"/test_pdf_bin.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll = [\"nll\"]\n",
    "acc = [\"acc\"]\n",
    "qwk = [\"qwk\"]\n",
    "nll_bin = [\"nll_bin\"]\n",
    "acc_bin = [\"acc_bin\"]\n",
    "bs = [\"bs\"]\n",
    "auc = [\"auc\"]\n",
    "for folder_name in folder_names:\n",
    "    pdf = pd.read_csv(OUTPUT_DIR + folder_name + \"/test_pdf.csv\")\n",
    "    pdf_bin = pd.read_csv(OUTPUT_DIR + folder_name + \"/test_pdf_bin.csv\")\n",
    "    \n",
    "    nll.append(skm.log_loss(pdf.y_true, pdf[[\"y_pred0\", \"y_pred1\", \"y_pred2\", \"y_pred3\", \"y_pred4\", \"y_pred5\", \"y_pred6\"]]))\n",
    "    acc.append(skm.accuracy_score(pdf.y_true, pdf.y_pred))\n",
    "    qwk.append(skm.cohen_kappa_score(pdf.y_true, pdf.y_pred, labels = [0,1,2,3,4,5,6], weights = \"quadratic\"))\n",
    "    \n",
    "    nll_bin.append(skm.log_loss(pdf_bin.y_true, pdf_bin[[\"y_pred0_unfav\",\"y_pred1_fav\"]]))\n",
    "    acc_bin.append(skm.accuracy_score(pdf_bin.y_true, pdf_bin.y_pred))\n",
    "    bs.append(skm.brier_score_loss(y_true = pdf_bin.y_true, y_prob = pdf_bin.y_pred1_fav, pos_label = 1))\n",
    "    auc.append(skm.roc_auc_score(y_true = pdf_bin.y_true, y_score = pdf_bin.y_pred1_fav))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal outcome\n",
    "header = [\" \", \"SI_CSb_TMAX\", \"SI_CSb_TMAX_DWI\"]\n",
    "data = [nll, acc, qwk]\n",
    "print(tabulate(data, headers = header, tablefmt = \"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Binary outcome\n",
    "header = [\" \", \"SI_CSb_TMAX\", \"SI_CSb_TMAX_DWI\"]\n",
    "data = [nll_bin, acc_bin, bs, auc]\n",
    "print(tabulate(data, headers = header, tablefmt = \"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Estimates\n",
    "folder_names = [\"SI_LSx\", \"SI_LSx_Expert\", \"SI_LSx_CSb\"]\n",
    "model_est = []\n",
    "for folder_name in folder_names:\n",
    "    cv_est = []\n",
    "    for f in range(5):\n",
    "        checkpoint_filepath = OUTPUT_DIR + folder_name + \"/fold\" + str(f) + \"/\"\n",
    "        est = pd.read_csv(checkpoint_filepath + \"estimates.csv\")\n",
    "        cv_est.append(est)\n",
    "    cv_est = pd.concat(cv_est)\n",
    "    model_est.append(cv_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates = pd.concat([pd.DataFrame(model_est[0].mean()), pd.DataFrame(model_est[1].mean()), pd.DataFrame(model_est[2].mean())], axis = 1)\n",
    "estimates.columns = folder_names\n",
    "estimates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
